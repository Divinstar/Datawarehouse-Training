{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c7a6e2-45e0-4ecd-a5fe-736635a63e5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2209457512301869#setting/sparkui/0611-043336-syhwrm2t/driver-1094461714714331476\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc7c2b91490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"FirstNotebook\").getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b02da8b3-6233-4964-b71b-d2650d548a75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+\n",
      "| ID|   Name| Department|Salary|\n",
      "+---+-------+-----------+------+\n",
      "|  1|  Alice|Engineering| 70000|\n",
      "|  2|    Bob|         HR| 48000|\n",
      "|  3|Charlie|  Marketing| 52000|\n",
      "|  4|  David|Engineering| 63000|\n",
      "|  5|    Eve|  Marketing| 45000|\n",
      "|  6|  Frank|Engineering| 59000|\n",
      "+---+-------+-----------+------+\n",
      "\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 70000),\n",
    "    (2, \"Bob\", \"HR\", 48000),\n",
    "    (3, \"Charlie\", \"Marketing\", 52000),\n",
    "    (4, \"David\", \"Engineering\", 63000),\n",
    "    (5, \"Eve\", \"Marketing\", 45000),\n",
    "    (6, \"Frank\", \"Engineering\", 59000),\n",
    "]\n",
    "\n",
    "columns = [\"ID\", \"Name\", \"Department\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Exercise Set 1: Basics\n",
    "# 1. Display all records in the DataFrame.\n",
    "# 2. Print the schema of the DataFrame.\n",
    "# 3. Count total number of employees.\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100f714d-4882-4a1d-b6b5-0ddf2b3ac1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+-------+\n",
      "| ID|   Name| Department|Salary|  Bonus|\n",
      "+---+-------+-----------+------+-------+\n",
      "|  1|  Alice|Engineering| 70000|10500.0|\n",
      "|  2|    Bob|         HR| 48000| 7200.0|\n",
      "|  3|Charlie|  Marketing| 52000| 7800.0|\n",
      "|  4|  David|Engineering| 63000| 9450.0|\n",
      "|  5|    Eve|  Marketing| 45000| 6750.0|\n",
      "|  6|  Frank|Engineering| 59000| 8850.0|\n",
      "+---+-------+-----------+------+-------+\n",
      "\n",
      "+---+-------+-----------+------+-------+-------+\n",
      "| ID|   Name| Department|Salary|  Bonus| NetPay|\n",
      "+---+-------+-----------+------+-------+-------+\n",
      "|  1|  Alice|Engineering| 70000|10500.0|80500.0|\n",
      "|  2|    Bob|         HR| 48000| 7200.0|55200.0|\n",
      "|  3|Charlie|  Marketing| 52000| 7800.0|59800.0|\n",
      "|  4|  David|Engineering| 63000| 9450.0|72450.0|\n",
      "|  5|    Eve|  Marketing| 45000| 6750.0|51750.0|\n",
      "|  6|  Frank|Engineering| 59000| 8850.0|67850.0|\n",
      "+---+-------+-----------+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Exercise Set 2: Column Operations\n",
    "# 4. Add a new column Bonus which is 15% of Salary.\n",
    "# 5. Add a new column NetPay = Salary + Bonus.\n",
    "\n",
    "bonus = df.withColumn(\"Bonus\", col(\"Salary\") * 0.15)\n",
    "bonus.show()\n",
    "\n",
    "netpay = bonus.withColumn(\"NetPay\", col(\"Salary\") + col(\"Bonus\"))\n",
    "netpay.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e368141-5ef1-4115-90c0-b64a0013b6ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+\n",
      "| ID| Name| Department|Salary|\n",
      "+---+-----+-----------+------+\n",
      "|  1|Alice|Engineering| 70000|\n",
      "|  4|David|Engineering| 63000|\n",
      "|  6|Frank|Engineering| 59000|\n",
      "+---+-----+-----------+------+\n",
      "\n",
      "+---+-----+-----------+------+\n",
      "| ID| Name| Department|Salary|\n",
      "+---+-----+-----------+------+\n",
      "|  1|Alice|Engineering| 70000|\n",
      "|  4|David|Engineering| 63000|\n",
      "+---+-----+-----------+------+\n",
      "\n",
      "+---+-----+-----------+------+\n",
      "| ID| Name| Department|Salary|\n",
      "+---+-----+-----------+------+\n",
      "|  1|Alice|Engineering| 70000|\n",
      "|  2|  Bob|         HR| 48000|\n",
      "|  4|David|Engineering| 63000|\n",
      "|  6|Frank|Engineering| 59000|\n",
      "+---+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise Set 3: Filtering and Conditions\n",
    "# 6. Display only employees from the “Engineering” department.\n",
    "# 7. Display employees whose salary is greater than 60000.\n",
    "# 8. Display employees who are not in the “Marketing” department.\n",
    "\n",
    "df.filter(col(\"Department\") == \"Engineering\").show()\n",
    "df.filter(col(\"Salary\") > 60000).show()\n",
    "df.filter(col(\"Department\") != \"Marketing\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4e9a173-3a8f-4bd5-9b9b-5802ac7f0ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+\n",
      "| ID| Name| Department|Salary|\n",
      "+---+-----+-----------+------+\n",
      "|  1|Alice|Engineering| 70000|\n",
      "|  4|David|Engineering| 63000|\n",
      "|  6|Frank|Engineering| 59000|\n",
      "+---+-----+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+-------+-----------+------+\n",
      "| ID|   Name| Department|Salary|\n",
      "+---+-------+-----------+------+\n",
      "|  1|  Alice|Engineering| 70000|\n",
      "|  4|  David|Engineering| 63000|\n",
      "|  6|  Frank|Engineering| 59000|\n",
      "|  2|    Bob|         HR| 48000|\n",
      "|  3|Charlie|  Marketing| 52000|\n",
      "|  5|    Eve|  Marketing| 45000|\n",
      "+---+-------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise Set 4: Sorting and Limiting\n",
    "# 9. Show top 3 highest paid employees.\n",
    "# 10. Sort the data by Department ascending and Salary descending.\n",
    "\n",
    "df.orderBy(col(\"Salary\").desc()).show(3)\n",
    "df.orderBy(col(\"Department\").asc(), col(\"Salary\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07339041-430c-4768-8c9e-131e7def9f31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+------+\n",
      "| ID|   Name| Department|Salary| Level|\n",
      "+---+-------+-----------+------+------+\n",
      "|  1|  Alice|Engineering| 70000|Senior|\n",
      "|  2|    Bob|         HR| 48000|Junior|\n",
      "|  3|Charlie|  Marketing| 52000|   Mid|\n",
      "|  4|  David|Engineering| 63000|Senior|\n",
      "|  5|    Eve|  Marketing| 45000|Junior|\n",
      "|  6|  Frank|Engineering| 59000|   Mid|\n",
      "+---+-------+-----------+------+------+\n",
      "\n",
      "+---+-------+-----------+------+----------+\n",
      "| ID|   Name| Department|Salary|Name_Upper|\n",
      "+---+-------+-----------+------+----------+\n",
      "|  1|  Alice|Engineering| 70000|     ALICE|\n",
      "|  2|    Bob|         HR| 48000|       BOB|\n",
      "|  3|Charlie|  Marketing| 52000|   CHARLIE|\n",
      "|  4|  David|Engineering| 63000|     DAVID|\n",
      "|  5|    Eve|  Marketing| 45000|       EVE|\n",
      "|  6|  Frank|Engineering| 59000|     FRANK|\n",
      "+---+-------+-----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise Set 5: String and Case Logic\n",
    "# 11. Add a new column Level :\n",
    "# “Senior” if salary > 60000\n",
    "# “Mid” if salary between 50000 and 60000\n",
    "# “Junior” otherwise\n",
    "# 12. Convert all names to uppercase.\n",
    "\n",
    "from pyspark.sql.functions import when, upper\n",
    "\n",
    "level = df.withColumn(\n",
    "    \"Level\",\n",
    "    when(col(\"Salary\") > 60000, \"Senior\")\n",
    "    .when((col(\"Salary\") >= 50000) & (col(\"Salary\") <= 60000), \"Mid\")\n",
    "    .otherwise(\"Junior\")\n",
    ")\n",
    "level.show()\n",
    "\n",
    "uppercase = df.withColumn(\"Name_Upper\", upper(col(\"Name\")))\n",
    "uppercase.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-06-10 17:10:16",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
