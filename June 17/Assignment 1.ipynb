{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gFiUf3JYWcF8"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q pyspark==3.5.1 delta-spark==3.1.0\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"ECommerceDeltaLake\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "orders_df = spark.read.csv(\"orders.csv\", header=True, inferSchema=True)\n",
        "customers_df = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
        "products_df = spark.read.csv(\"products.csv\", header=True, inferSchema=True)\n",
        "\n",
        "orders_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/delta/orders\")\n",
        "customers_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/delta/customers\")\n",
        "products_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/delta/products\")\n"
      ],
      "metadata": {
        "id": "LHdTKe61XaW_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Ingest all 3 CSVs as Delta Tables.\n",
        "spark.read.format(\"delta\").load(\"/content/delta/orders\").createOrReplaceTempView(\"orders\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT ProductID, SUM(Quantity * Price) AS TotalRevenue\n",
        "FROM orders\n",
        "WHERE Status = 'Delivered'\n",
        "GROUP BY ProductID\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ablZ-bIiXaf0",
        "outputId": "3fa5d045-ec25-4387-ce1e-cfea416ecc51"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|ProductID|TotalRevenue|\n",
            "+---------+------------+\n",
            "|    P1001|       75000|\n",
            "|    P1002|       50000|\n",
            "|    P1003|       30000|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Write SQL to get the total revenue per Product.\n",
        "spark.read.format(\"delta\").load(\"/content/delta/orders\").createOrReplaceTempView(\"orders\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT ProductID, SUM(Quantity * Price) AS TotalRevenue\n",
        "FROM orders\n",
        "WHERE Status = 'Delivered'\n",
        "GROUP BY ProductID\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEfnaO5BYPxg",
        "outputId": "5d80731e-0f17-4d10-959b-baf428d83d8e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|ProductID|TotalRevenue|\n",
            "+---------+------------+\n",
            "|    P1001|       75000|\n",
            "|    P1002|       50000|\n",
            "|    P1003|       30000|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Join Orders + Customers to find revenue by Region\n",
        "orders_df = spark.read.format(\"delta\").load(\"/content/delta/orders\")\n",
        "customers_df = spark.read.format(\"delta\").load(\"/content/delta/customers\")\n",
        "\n",
        "orders_df.createOrReplaceTempView(\"orders\")\n",
        "customers_df.createOrReplaceTempView(\"customers\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT c.Region, SUM(o.Quantity * o.Price) AS RegionalRevenue\n",
        "FROM orders o\n",
        "JOIN customers c ON o.CustomerID = c.CustomerID\n",
        "WHERE o.Status = 'Delivered'\n",
        "GROUP BY c.Region\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIKykWFbYaQH",
        "outputId": "e21c5a89-aaa9-45ac-fd93-74cefa80710c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+\n",
            "|Region|RegionalRevenue|\n",
            "+------+---------------+\n",
            "|  West|          30000|\n",
            "| North|         125000|\n",
            "+------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.Update the Status of Pending orders to 'Cancelled'\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "orders_delta = DeltaTable.forPath(spark, \"/content/delta/orders\")\n",
        "\n",
        "orders_delta.update(\n",
        "    condition=\"Status = 'Pending'\",\n",
        "    set={\"Status\": \"'Cancelled'\"}\n",
        ")\n",
        "\n",
        "orders_delta.toDF().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSXsI6-RYcjh",
        "outputId": "280ab620-a0a1-48b7-86b8-e920e4c3de39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|Cancelled|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Merge a new return record into Orders\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "new_order = spark.createDataFrame([\n",
        "    (3006, \"C002\", \"P1002\", 1, 50000, \"2024-05-06\", \"Returned\")\n",
        "], [\"OrderID\", \"CustomerID\", \"ProductID\", \"Quantity\", \"Price\", \"OrderDate\", \"Status\"])\n",
        "\n",
        "orders_delta.alias(\"target\").merge(\n",
        "    new_order.alias(\"source\"),\n",
        "    \"target.OrderID = source.OrderID\"\n",
        ").whenNotMatchedInsertAll().execute()\n",
        "\n",
        "orders_delta.toDF().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPixKyneYcow",
        "outputId": "0a5c187b-70e2-4c70-f4fd-24e516f36a3c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|Cancelled|\n",
            "|   3006|      C002|    P1002|       1|50000|2024-05-06| Returned|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.Create raw → cleaned → aggregated tables\n",
        "raw_orders = spark.read.format(\"delta\").load(\"/content/delta/orders\")\n",
        "\n",
        "cleaned_orders = raw_orders.dropna()\n",
        "cleaned_orders.write.format(\"delta\").mode(\"overwrite\").save(\"/content/delta/cleaned_orders\")\n"
      ],
      "metadata": {
        "id": "Odk19M2MYctK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. View previous version\n",
        "old_orders = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/content/delta/orders\")\n",
        "old_orders.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMGDWre6YcxL",
        "outputId": "7a1c0154-80e0-4ee2-82a1-0cf2450e600c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. restore to older versions\n",
        "original_orders = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/content/delta/orders\")\n",
        "original_orders.write.format(\"delta\").mode(\"overwrite\").save(\"/content/delta/orders\")\n"
      ],
      "metadata": {
        "id": "VVdOxccYYc1N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.\n",
        "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\n",
        "\n",
        "from delta.tables import DeltaTable\n",
        "orders_table = DeltaTable.forPath(spark, \"/content/delta/orders\")\n",
        "orders_table.vacuum(0)\n",
        "\n",
        "\n",
        "orders_table.toDF().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLaW2n-ZYc5C",
        "outputId": "33f0155e-3909-486e-f2c0-bd02f84f8d5c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Expectations: Quantity > 0, Price > 0, OrderDate not null\n",
        "orders = spark.read.format(\"delta\").load(\"/content/delta/orders\")\n",
        "\n",
        "valid_orders = orders.filter(\"Quantity > 0 AND Price > 0 AND OrderDate IS NOT NULL\")\n",
        "valid_orders.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O4EHVjIYc87",
        "outputId": "3cdc2de7-386a-4910-ed0a-1a5be2c7f501"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  11. Bonus: Add OrderType column using when-otherwise\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "orders = spark.read.format(\"delta\").load(\"/content/delta/orders\")\n",
        "\n",
        "orders_with_type = orders.withColumn(\"OrderType\", when(orders.Status == \"Returned\", \"Return\").otherwise(\"Regular\"))\n",
        "orders_with_type.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hgSO67AYdBG",
        "outputId": "fc33be58-acff-4780-eb96-ec4fed84cf1b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|OrderType|\n",
            "+-------+----------+---------+--------+-----+----------+---------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|  Regular|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|   Return|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|  Regular|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|  Regular|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|  Regular|\n",
            "+-------+----------+---------+--------+-----+----------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KGVcXYWtYdEv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}